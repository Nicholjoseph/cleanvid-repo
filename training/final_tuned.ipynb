{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "136aa64caf134f44b23d446dfbeebe8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4266df222f274112a67df170bd10e507",
              "IPY_MODEL_ac3820325e1e4c64896702ea0407d22b",
              "IPY_MODEL_1dd6ad18001642bfb5681b77d29cbf37"
            ],
            "layout": "IPY_MODEL_a2d58464f8084e8aa3130181230b63dd"
          }
        },
        "4266df222f274112a67df170bd10e507": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_637831442c1a47a4875e1f23fbdc3c14",
            "placeholder": "​",
            "style": "IPY_MODEL_84cce0ccb789471c9541154fe2679d78",
            "value": "100%"
          }
        },
        "ac3820325e1e4c64896702ea0407d22b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b490fd08a334e18b8abb8d1b6efc9bf",
            "max": 24783,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee342abbd1eb4542a35c84e0bc03dcb1",
            "value": 24783
          }
        },
        "1dd6ad18001642bfb5681b77d29cbf37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b63b345cca141bebecc4ab7cd0825d2",
            "placeholder": "​",
            "style": "IPY_MODEL_73679cc02035488eafb5f230dfeab448",
            "value": " 24783/24783 [00:04&lt;00:00, 4416.18it/s]"
          }
        },
        "a2d58464f8084e8aa3130181230b63dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "637831442c1a47a4875e1f23fbdc3c14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84cce0ccb789471c9541154fe2679d78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b490fd08a334e18b8abb8d1b6efc9bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee342abbd1eb4542a35c84e0bc03dcb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b63b345cca141bebecc4ab7cd0825d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73679cc02035488eafb5f230dfeab448": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6282b1aa90d4ae4ba111d2fb4dd9cb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8663ea8d2b704efa981168a9fab5f5a0",
              "IPY_MODEL_a2c3ca22057f412bab66a8e8ac0ce46e",
              "IPY_MODEL_70d1a071e4194706ad357691db795845"
            ],
            "layout": "IPY_MODEL_460cbc5ee0de4be696f3561420334b2e"
          }
        },
        "8663ea8d2b704efa981168a9fab5f5a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1c1bf1fc2184349b5176fc9883eeffc",
            "placeholder": "​",
            "style": "IPY_MODEL_66e93998e4154522b27e5bb865797c64",
            "value": "Map: 100%"
          }
        },
        "a2c3ca22057f412bab66a8e8ac0ce46e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8f8c7fcd0ce4f5dbf02b12c8410d4f9",
            "max": 44008,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca23f54e76a943588d51e2db87b25e1d",
            "value": 44008
          }
        },
        "70d1a071e4194706ad357691db795845": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_216af7a025da45659fbf3dc71bf06d02",
            "placeholder": "​",
            "style": "IPY_MODEL_c61d7afe393b4ff6a0f620f818749704",
            "value": " 44008/44008 [00:06&lt;00:00, 7039.87 examples/s]"
          }
        },
        "460cbc5ee0de4be696f3561420334b2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1c1bf1fc2184349b5176fc9883eeffc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66e93998e4154522b27e5bb865797c64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8f8c7fcd0ce4f5dbf02b12c8410d4f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca23f54e76a943588d51e2db87b25e1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "216af7a025da45659fbf3dc71bf06d02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c61d7afe393b4ff6a0f620f818749704": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4595d6e4670147a688e99a89c283d241": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_553fa631a74d42398a041c8da16f4dad",
              "IPY_MODEL_39cb41629b7949e099113b24d4b0b039",
              "IPY_MODEL_828fdca39d0b435f9e0733ecf0a113d1"
            ],
            "layout": "IPY_MODEL_ea01b4f7801647a188bcfb03b077b195"
          }
        },
        "553fa631a74d42398a041c8da16f4dad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61c38fbe359f4b24877d33b4bbaddfc0",
            "placeholder": "​",
            "style": "IPY_MODEL_1971aba2d9d648f0862b7b120f9dab09",
            "value": "Map: 100%"
          }
        },
        "39cb41629b7949e099113b24d4b0b039": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f305b62305e04e8cb352a46fc1d8fc76",
            "max": 11003,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2578354ae8264230bdcb3cf4f66afe61",
            "value": 11003
          }
        },
        "828fdca39d0b435f9e0733ecf0a113d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87a08a806c224ede8e42d11a8cce5c4b",
            "placeholder": "​",
            "style": "IPY_MODEL_a8ac65bf613d4573b5ae81ca252d61e6",
            "value": " 11003/11003 [00:02&lt;00:00, 8046.19 examples/s]"
          }
        },
        "ea01b4f7801647a188bcfb03b077b195": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61c38fbe359f4b24877d33b4bbaddfc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1971aba2d9d648f0862b7b120f9dab09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f305b62305e04e8cb352a46fc1d8fc76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2578354ae8264230bdcb3cf4f66afe61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "87a08a806c224ede8e42d11a8cce5c4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8ac65bf613d4573b5ae81ca252d61e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./offensive_word_nlp_model\n",
        "!rm -rf ./offensive_word_nlp_model_final\n",
        "!rm -rf ./logs"
      ],
      "metadata": {
        "id": "FKO7pYJDcQ31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "136aa64caf134f44b23d446dfbeebe8c",
            "4266df222f274112a67df170bd10e507",
            "ac3820325e1e4c64896702ea0407d22b",
            "1dd6ad18001642bfb5681b77d29cbf37",
            "a2d58464f8084e8aa3130181230b63dd",
            "637831442c1a47a4875e1f23fbdc3c14",
            "84cce0ccb789471c9541154fe2679d78",
            "4b490fd08a334e18b8abb8d1b6efc9bf",
            "ee342abbd1eb4542a35c84e0bc03dcb1",
            "5b63b345cca141bebecc4ab7cd0825d2",
            "73679cc02035488eafb5f230dfeab448",
            "b6282b1aa90d4ae4ba111d2fb4dd9cb6",
            "8663ea8d2b704efa981168a9fab5f5a0",
            "a2c3ca22057f412bab66a8e8ac0ce46e",
            "70d1a071e4194706ad357691db795845",
            "460cbc5ee0de4be696f3561420334b2e",
            "a1c1bf1fc2184349b5176fc9883eeffc",
            "66e93998e4154522b27e5bb865797c64",
            "d8f8c7fcd0ce4f5dbf02b12c8410d4f9",
            "ca23f54e76a943588d51e2db87b25e1d",
            "216af7a025da45659fbf3dc71bf06d02",
            "c61d7afe393b4ff6a0f620f818749704",
            "4595d6e4670147a688e99a89c283d241",
            "553fa631a74d42398a041c8da16f4dad",
            "39cb41629b7949e099113b24d4b0b039",
            "828fdca39d0b435f9e0733ecf0a113d1",
            "ea01b4f7801647a188bcfb03b077b195",
            "61c38fbe359f4b24877d33b4bbaddfc0",
            "1971aba2d9d648f0862b7b120f9dab09",
            "f305b62305e04e8cb352a46fc1d8fc76",
            "2578354ae8264230bdcb3cf4f66afe61",
            "87a08a806c224ede8e42d11a8cce5c4b",
            "a8ac65bf613d4573b5ae81ca252d61e6"
          ]
        },
        "id": "LHuOe5XzVYjX",
        "outputId": "7481584c-4177-4804-aee4-c7ea88681fbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading NLTK resources...\n",
            "Error with NLTK download: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Implementing a simple fallback tokenizer...\n",
            "Using device: cuda\n",
            "Setting up NLP-based offensive word detection system...\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['count', 'hate_speech_count', 'offensive_language_count', 'neither_count', 'class', 'tweet'],\n",
            "        num_rows: 24783\n",
            "    })\n",
            "})\n",
            "Creating word-level dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/24783 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "136aa64caf134f44b23d446dfbeebe8c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created word-level dataset with 55011 samples\n",
            "Offensive samples: 18337\n",
            "Non-offensive samples: 36674\n",
            "Training data: 44008 samples\n",
            "Evaluation data: 11003 samples\n",
            "Setting up pre-trained model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/44008 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6282b1aa90d4ae4ba111d2fb4dd9cb6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/11003 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4595d6e4670147a688e99a89c283d241"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-6-a5e324a50809>:189: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting NLP model fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8253' max='8253' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8253/8253 15:04, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.315000</td>\n",
              "      <td>0.296868</td>\n",
              "      <td>0.872853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.215300</td>\n",
              "      <td>0.304487</td>\n",
              "      <td>0.887758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.160600</td>\n",
              "      <td>0.383529</td>\n",
              "      <td>0.891393</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLP model saved to: ./offensive_word_nlp_model_final\n",
            "\n",
            "Testing the NLP model with a sample text...\n",
            "Found 7 potentially offensive words:\n",
            "- 'stuff' (confidence: 0.88)\n",
            "  Context: \"this damn stuff terrible and pisses\"\n",
            "- 'terrible' (confidence: 0.84)\n",
            "  Context: \"this damn stuff terrible and pisses off\"\n",
            "- 'and' (confidence: 0.79)\n",
            "  Context: \"damn stuff terrible and pisses off can\"\n",
            "- 'can' (confidence: 0.99)\n",
            "  Context: \"and pisses off can stand this shit\"\n",
            "- 'stand' (confidence: 0.99)\n",
            "  Context: \"pisses off can stand this shit\"\n",
            "- 'this' (confidence: 0.99)\n",
            "  Context: \"off can stand this shit\"\n",
            "- 'shit' (confidence: 0.99)\n",
            "  Context: \"can stand this shit\"\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install transformers datasets evaluate scikit-learn torch nltk pandas tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from datasets import load_dataset, Dataset\n",
        "import evaluate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "\n",
        "# Fix 1: More robust NLTK download\n",
        "print(\"Downloading NLTK resources...\")\n",
        "try:\n",
        "    # Create nltk_data directory if it doesn't exist\n",
        "    nltk_data_dir = os.path.expanduser('~/nltk_data')\n",
        "    os.makedirs(nltk_data_dir, exist_ok=True)\n",
        "\n",
        "    # Download punkt to the specified directory\n",
        "    nltk.download('punkt', download_dir=nltk_data_dir)\n",
        "\n",
        "    # Test the tokenizer\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    test = word_tokenize(\"Test sentence.\")\n",
        "    print(\"NLTK punkt tokenizer successfully loaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error with NLTK download: {e}\")\n",
        "    print(\"Implementing a simple fallback tokenizer...\")\n",
        "    # Simple fallback tokenizer function\n",
        "    def word_tokenize(text):\n",
        "        # Remove punctuation that's connected to words\n",
        "        for punct in \".,!?;:'\\\")]}-_\":\n",
        "            text = text.replace(punct, ' ')\n",
        "        for punct in \"[({\\\"-_\":\n",
        "            text = text.replace(punct, ' ')\n",
        "        # Split by whitespace\n",
        "        return text.split()\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "print(\"Setting up NLP-based offensive word detection system...\")\n",
        "\n",
        "# 1. Load and prepare the dataset\n",
        "print(\"Loading dataset...\")\n",
        "dataset = load_dataset(\"hate_speech_offensive\")\n",
        "print(f\"Dataset loaded: {dataset}\")\n",
        "\n",
        "# 2. Prepare word-level training data\n",
        "def create_word_level_dataset():\n",
        "    \"\"\"Create a dataset of individual words with offensive/non-offensive labels\"\"\"\n",
        "    print(\"Creating word-level dataset...\")\n",
        "\n",
        "    # Extract words and their context from the dataset\n",
        "    all_words = []\n",
        "    word_contexts = []\n",
        "    word_labels = []\n",
        "\n",
        "    # Process each tweet\n",
        "    for example in tqdm(dataset[\"train\"]):\n",
        "        tweet = example[\"tweet\"]\n",
        "        # 0 = hate speech, 1 = offensive language, 2 = neither\n",
        "        is_offensive = 1 if example[\"class\"] in [0, 1] else 0\n",
        "\n",
        "        # Tokenize the tweet\n",
        "        words = word_tokenize(tweet.lower())\n",
        "\n",
        "        # For each word, create a context window (the word with surrounding words)\n",
        "        for i, word in enumerate(words):\n",
        "            if len(word) <= 2 or not word.isalpha():\n",
        "                continue\n",
        "\n",
        "            # Get context (up to 3 words before and after)\n",
        "            start = max(0, i - 3)\n",
        "            end = min(len(words), i + 4)\n",
        "            context = \" \".join(words[start:end])\n",
        "\n",
        "            all_words.append(word)\n",
        "            word_contexts.append(context)\n",
        "            word_labels.append(is_offensive)\n",
        "\n",
        "    # Create a DataFrame\n",
        "    word_df = pd.DataFrame({\n",
        "        \"word\": all_words,\n",
        "        \"context\": word_contexts,\n",
        "        \"offensive\": word_labels\n",
        "    })\n",
        "\n",
        "    # Calculate word frequency and offensive ratio\n",
        "    word_stats = word_df.groupby(\"word\").agg({\n",
        "        \"offensive\": [\"count\", \"mean\"]\n",
        "    }).reset_index()\n",
        "    word_stats.columns = [\"word\", \"count\", \"offensive_ratio\"]\n",
        "\n",
        "    # Filter for words that appear at least 5 times\n",
        "    filtered_words = word_stats[word_stats[\"count\"] >= 5][\"word\"].tolist()\n",
        "    filtered_df = word_df[word_df[\"word\"].isin(filtered_words)]\n",
        "\n",
        "    # Create balanced dataset by sampling\n",
        "    offensive_samples = filtered_df[filtered_df[\"offensive\"] == 1]\n",
        "    non_offensive_samples = filtered_df[filtered_df[\"offensive\"] == 0]\n",
        "\n",
        "    # Balance the dataset if needed\n",
        "    if len(offensive_samples) < len(non_offensive_samples):\n",
        "        non_offensive_samples = non_offensive_samples.sample(\n",
        "            n=len(offensive_samples) * 2,\n",
        "            random_state=42\n",
        "        )\n",
        "    else:\n",
        "        offensive_samples = offensive_samples.sample(\n",
        "            n=len(non_offensive_samples) // 2,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    # Combine and shuffle\n",
        "    balanced_df = pd.concat([offensive_samples, non_offensive_samples])\n",
        "    balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    print(f\"Created word-level dataset with {len(balanced_df)} samples\")\n",
        "    print(f\"Offensive samples: {len(balanced_df[balanced_df['offensive'] == 1])}\")\n",
        "    print(f\"Non-offensive samples: {len(balanced_df[balanced_df['offensive'] == 0])}\")\n",
        "\n",
        "    return balanced_df\n",
        "\n",
        "# Create the word-level dataset\n",
        "word_dataset_df = create_word_level_dataset()\n",
        "\n",
        "# Split into train and evaluation sets\n",
        "train_df, eval_df = train_test_split(\n",
        "    word_dataset_df, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training data: {len(train_df)} samples\")\n",
        "print(f\"Evaluation data: {len(eval_df)} samples\")\n",
        "\n",
        "# Convert to Datasets format\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "eval_dataset = Dataset.from_pandas(eval_df)\n",
        "\n",
        "# 3. Load a pre-trained model and tokenizer\n",
        "print(\"Setting up pre-trained model...\")\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# Fix 2: Move model to the correct device\n",
        "model = model.to(device)\n",
        "\n",
        "# 4. Prepare the data for fine-tuning\n",
        "def preprocess_function(examples):\n",
        "    # Use the context for better performance\n",
        "    return tokenizer(examples[\"context\"], truncation=True, padding=\"max_length\", max_length=64)\n",
        "\n",
        "# Apply tokenization\n",
        "print(\"Tokenizing dataset...\")\n",
        "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_eval = eval_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# 5. Define metrics for evaluation\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# 6. Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./offensive_word_nlp_model\",\n",
        "    report_to=\"none\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    logging_dir=\"./logs\",\n",
        ")\n",
        "\n",
        "# 7. Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train.select_columns([\"input_ids\", \"attention_mask\", \"offensive\"]).rename_column(\"offensive\", \"labels\"),\n",
        "    eval_dataset=tokenized_eval.select_columns([\"input_ids\", \"attention_mask\", \"offensive\"]).rename_column(\"offensive\", \"labels\"),\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# 8. Fine-tune the model\n",
        "print(\"Starting NLP model fine-tuning...\")\n",
        "trainer.train()\n",
        "\n",
        "# 9. Save the model\n",
        "model_path = \"./offensive_word_nlp_model_final\"\n",
        "trainer.save_model(model_path)\n",
        "tokenizer.save_pretrained(model_path)\n",
        "print(f\"NLP model saved to: {model_path}\")\n",
        "\n",
        "# 10. Create a function to detect offensive words using the NLP model - Fixed with device handling\n",
        "def detect_offensive_words_nlp(text, model, tokenizer, threshold=0.7):\n",
        "    \"\"\"\n",
        "    Detect offensive words in text using the NLP model\n",
        "\n",
        "    Args:\n",
        "        text: Text to analyze\n",
        "        model: Fine-tuned offensive language detection model\n",
        "        tokenizer: Tokenizer for the model\n",
        "        threshold: Confidence threshold to consider a word offensive\n",
        "\n",
        "    Returns:\n",
        "        List of offensive words and their scores\n",
        "    \"\"\"\n",
        "    # Ensure model is in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Get device that model is on\n",
        "    model_device = next(model.parameters()).device\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text.lower())\n",
        "\n",
        "    # Filter out non-alphabetic tokens and short words\n",
        "    words = [word for word in words if word.isalpha() and len(word) > 2]\n",
        "\n",
        "    # Process each word with context\n",
        "    results = []\n",
        "    for i, word in enumerate(words):\n",
        "        # Get context (up to 3 words before and after)\n",
        "        start = max(0, i - 3)\n",
        "        end = min(len(words), i + 4)\n",
        "        context = \" \".join(words[start:end])\n",
        "\n",
        "        # Use model to predict if the word is offensive in this context\n",
        "        inputs = tokenizer(context, return_tensors=\"pt\")\n",
        "\n",
        "        # Fix: Move inputs to the same device as the model\n",
        "        inputs = {k: v.to(model_device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(**inputs).logits\n",
        "\n",
        "        # Get prediction score (0 = not offensive, 1 = offensive)\n",
        "        scores = torch.softmax(logits, dim=1)[0].tolist()\n",
        "        offensive_score = scores[1]  # Probability of being offensive\n",
        "\n",
        "        if offensive_score >= threshold:\n",
        "            results.append({\n",
        "                \"word\": word,\n",
        "                \"context\": context,\n",
        "                \"confidence\": offensive_score\n",
        "            })\n",
        "\n",
        "    return results\n",
        "\n",
        "# 11. Test the NLP-based detection\n",
        "print(\"\\nTesting the NLP model with a sample text...\")\n",
        "sample_text = \"This damn stuff is terrible and pisses me off. I can't stand this shit.\"\n",
        "\n",
        "detected = detect_offensive_words_nlp(sample_text, model, tokenizer)\n",
        "\n",
        "print(f\"Found {len(detected)} potentially offensive words:\")\n",
        "for item in detected:\n",
        "    print(f\"- '{item['word']}' (confidence: {item['confidence']:.2f})\")\n",
        "    print(f\"  Context: \\\"{item['context']}\\\"\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "def test_offensive_word_detector():\n",
        "    \"\"\"Test if the NLP offensive word detector model works as expected\"\"\"\n",
        "    print(\"Testing the NLP offensive word detector...\")\n",
        "\n",
        "    # 1. Load the saved model and tokenizer\n",
        "    try:\n",
        "        model_path = \"./offensive_word_nlp_model_final\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "        print(\"✓ Model and tokenizer loaded successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Failed to load model: {e}\")\n",
        "        return False\n",
        "\n",
        "    # 2. Detect device and move model\n",
        "    try:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model = model.to(device)\n",
        "        print(f\"✓ Model moved to device: {device}\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Failed to move model to device: {e}\")\n",
        "        return False\n",
        "\n",
        "    # 3. Define test function\n",
        "    def detect_offensive_words(text, threshold=0.7):\n",
        "        \"\"\"Test function to detect offensive words\"\"\"\n",
        "        model.eval()\n",
        "        model_device = next(model.parameters()).device\n",
        "\n",
        "        # Simple word tokenization\n",
        "        words = text.lower().split()\n",
        "\n",
        "        results = []\n",
        "        for word in words:\n",
        "            if len(word) <= 2 or not any(c.isalpha() for c in word):\n",
        "                continue\n",
        "\n",
        "            # Context is just the word itself for this simple test\n",
        "            context = word\n",
        "\n",
        "            # Use model to predict\n",
        "            inputs = tokenizer(context, return_tensors=\"pt\")\n",
        "            inputs = {k: v.to(model_device) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = model(**inputs).logits\n",
        "\n",
        "            scores = torch.softmax(logits, dim=1)[0].tolist()\n",
        "            offensive_score = scores[1]  # Probability of being offensive\n",
        "\n",
        "            results.append({\n",
        "                \"word\": word,\n",
        "                \"confidence\": offensive_score,\n",
        "                \"is_offensive\": offensive_score >= threshold\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    # 4. Test with various sentences\n",
        "    test_sentences = [\n",
        "        \"This is a normal sentence without any offensive words.\",\n",
        "        \"This damn thing is really annoying me.\",\n",
        "        \"I love this fucking bitch and these wonderful flowers.\",\n",
        "        \"That stupid idiot made me so angry I could scream.\",\n",
        "    ]\n",
        "\n",
        "    # 5. Run tests and report results\n",
        "    print(\"\\n--- Testing with sample sentences ---\")\n",
        "    all_tests_passed = True\n",
        "\n",
        "    for sentence in test_sentences:\n",
        "        print(f\"\\nTesting: \\\"{sentence}\\\"\")\n",
        "        try:\n",
        "            results = detect_offensive_words(sentence)\n",
        "            print(\"Results:\")\n",
        "            for result in results:\n",
        "                status = \"OFFENSIVE\" if result[\"is_offensive\"] else \"clean\"\n",
        "                print(f\"- '{result['word']}': {result['confidence']:.2f} ({status})\")\n",
        "            print(\"✓ Sentence processed successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Failed to process sentence: {e}\")\n",
        "            all_tests_passed = False\n",
        "\n",
        "    # 6. Final status\n",
        "    if all_tests_passed:\n",
        "        print(\"\\n✅ All tests completed successfully! The model is working.\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"\\n❌ Some tests failed. Check the errors above.\")\n",
        "        return False\n",
        "\n",
        "# Run the test\n",
        "if __name__ == \"__main__\":\n",
        "    test_offensive_word_detector()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfLJ1hBzg30Q",
        "outputId": "12ffae75-4a9d-474d-f66d-974deba24eb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing the NLP offensive word detector...\n",
            "✓ Model and tokenizer loaded successfully\n",
            "✓ Model moved to device: cuda\n",
            "\n",
            "--- Testing with sample sentences ---\n",
            "\n",
            "Testing: \"This is a normal sentence without any offensive words.\"\n",
            "Results:\n",
            "- 'this': 0.34 (clean)\n",
            "- 'normal': 0.24 (clean)\n",
            "- 'sentence': 0.32 (clean)\n",
            "- 'without': 0.34 (clean)\n",
            "- 'any': 0.35 (clean)\n",
            "- 'offensive': 0.04 (clean)\n",
            "- 'words.': 0.35 (clean)\n",
            "✓ Sentence processed successfully\n",
            "\n",
            "Testing: \"This damn thing is really annoying me.\"\n",
            "Results:\n",
            "- 'this': 0.34 (clean)\n",
            "- 'damn': 0.89 (OFFENSIVE)\n",
            "- 'thing': 0.24 (clean)\n",
            "- 'really': 0.34 (clean)\n",
            "- 'annoying': 0.32 (clean)\n",
            "- 'me.': 0.45 (clean)\n",
            "✓ Sentence processed successfully\n",
            "\n",
            "Testing: \"I love this fucking bitch and these wonderful flowers.\"\n",
            "Results:\n",
            "- 'love': 0.36 (clean)\n",
            "- 'this': 0.34 (clean)\n",
            "- 'fucking': 0.99 (OFFENSIVE)\n",
            "- 'bitch': 1.00 (OFFENSIVE)\n",
            "- 'and': 0.31 (clean)\n",
            "- 'these': 0.34 (clean)\n",
            "- 'wonderful': 0.07 (clean)\n",
            "- 'flowers.': 0.11 (clean)\n",
            "✓ Sentence processed successfully\n",
            "\n",
            "Testing: \"That stupid idiot made me so angry I could scream.\"\n",
            "Results:\n",
            "- 'that': 0.35 (clean)\n",
            "- 'stupid': 0.53 (clean)\n",
            "- 'idiot': 0.33 (clean)\n",
            "- 'made': 0.16 (clean)\n",
            "- 'angry': 0.38 (clean)\n",
            "- 'could': 0.29 (clean)\n",
            "- 'scream.': 0.29 (clean)\n",
            "✓ Sentence processed successfully\n",
            "\n",
            "✅ All tests completed successfully! The model is working.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 1: In your notebook environment - Create a ZIP archive of the model\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def prepare_model_for_download():\n",
        "    \"\"\"\n",
        "    Prepares the offensive word NLP model for download by creating a ZIP archive\n",
        "    \"\"\"\n",
        "    model_path = \"./offensive_word_nlp_model_final\"\n",
        "    zip_path = \"./offensive_word_model.zip\"\n",
        "\n",
        "    # Check if model exists\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Error: Model directory '{model_path}' not found. Make sure the model has been trained.\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # Create a ZIP archive of the model directory\n",
        "        print(f\"Creating ZIP archive of model at {zip_path}...\")\n",
        "        shutil.make_archive(\"offensive_word_model\", 'zip', model_path)\n",
        "\n",
        "        print(f\"✅ Model archive created successfully: {zip_path}\")\n",
        "        print(\"You can now download this ZIP file to your local machine.\")\n",
        "\n",
        "        # If running in Google Colab, display download link\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            print(\"Download link ready. Click the link below:\")\n",
        "            files.download(zip_path)\n",
        "        except ImportError:\n",
        "            print(\"Not running in Google Colab. Use your platform's file download option to download the ZIP file.\")\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error creating ZIP archive: {e}\")\n",
        "        return False\n",
        "\n",
        "# Execute the function to prepare the model for download\n",
        "prepare_model_for_download()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "lWI4YUwWhY40",
        "outputId": "f3f20c6b-50cb-43da-985b-5fed43f3d4a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating ZIP archive of model at ./offensive_word_model.zip...\n",
            "✅ Model archive created successfully: ./offensive_word_model.zip\n",
            "You can now download this ZIP file to your local machine.\n",
            "Download link ready. Click the link below:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_59637b3f-8feb-4596-a38d-aea83d93804e\", \"offensive_word_model.zip\", 247313724)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import re\n",
        "import os\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Ensure NLTK punkt tokenizer data is downloaded\n",
        "try:\n",
        "    nltk.download('punkt', quiet=False)  # Set quiet=False to see detailed output\n",
        "    # Verify punkt data location\n",
        "    punkt_path = nltk.data.find('tokenizers/punkt')\n",
        "    print(f\"Punkt data location: {punkt_path}\")\n",
        "\n",
        "    # Check for the english.pickle file\n",
        "    punkt_tab_path = os.path.join(punkt_path, \"english.pickle\")\n",
        "    if os.path.exists(punkt_tab_path):\n",
        "        print(\"punkt_tab file found!\")\n",
        "    else:\n",
        "        print(\"punkt_tab file not found! Manually downloading and installing...\")\n",
        "        # Manually download punkt data\n",
        "        nltk.download('punkt', download_dir='/root/nltk_data')\n",
        "        print(\"punkt data downloaded and installed successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading or verifying NLTK punkt data: {e}\")\n",
        "    raise\n",
        "\n",
        "# Manually load the punkt tokenizer\n",
        "try:\n",
        "    from nltk.tokenize import PunktSentenceTokenizer\n",
        "    punkt_tokenizer = PunktSentenceTokenizer()\n",
        "    print(\"Punkt tokenizer loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading punkt tokenizer: {e}\")\n",
        "    raise\n",
        "\n",
        "class WhisperTranscriptProcessor:\n",
        "    def __init__(self, model_path, threshold=0.7):\n",
        "        \"\"\"\n",
        "        Initialize the transcript processor with the offensive word detection model\n",
        "\n",
        "        Args:\n",
        "            model_path: Path to the trained offensive word detection model\n",
        "            threshold: Confidence threshold for offensive word detection\n",
        "        \"\"\"\n",
        "        # Use NLTK's word_tokenize for tokenization\n",
        "        self.word_tokenize = word_tokenize\n",
        "\n",
        "        # Load model and tokenizer\n",
        "        self.threshold = threshold\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "        # Move model to GPU if available\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()  # Set to evaluation mode\n",
        "        print(f\"Model loaded successfully and moved to {self.device}\")\n",
        "\n",
        "    def parse_whisper_transcript(self, transcript_file):\n",
        "        \"\"\"\n",
        "        Parse the Whisper transcript file to extract segments with timestamps\n",
        "\n",
        "        Args:\n",
        "            transcript_file: Path to the Whisper transcript file\n",
        "\n",
        "        Returns:\n",
        "            List of segments with text and timestamps\n",
        "        \"\"\"\n",
        "        segments = []\n",
        "\n",
        "        # Check the file format (JSON or text)\n",
        "        if transcript_file.endswith('.json'):\n",
        "            # Parse JSON format\n",
        "            with open(transcript_file, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            # Handle different Whisper JSON formats\n",
        "            if 'segments' in data:\n",
        "                # Standard Whisper JSON format\n",
        "                for segment in data['segments']:\n",
        "                    segments.append({\n",
        "                        'text': segment['text'].strip(),\n",
        "                        'start': segment['start'],\n",
        "                        'end': segment['end']\n",
        "                    })\n",
        "            else:\n",
        "                # Simple format with just timestamps and text\n",
        "                for segment in data:\n",
        "                    if 'text' in segment and 'start' in segment and 'end' in segment:\n",
        "                        segments.append({\n",
        "                            'text': segment['text'].strip(),\n",
        "                            'start': segment['start'],\n",
        "                            'end': segment['end']\n",
        "                        })\n",
        "        else:\n",
        "            # Parse text format (assuming HH:MM:SS.mmm --> HH:MM:SS.mmm format)\n",
        "            pattern = r'(\\d+:\\d+:\\d+\\.\\d+) --> (\\d+:\\d+:\\d+\\.\\d+)\\s*\\n(.*?)(?:\\n\\n|\\Z)'\n",
        "\n",
        "            with open(transcript_file, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            matches = re.findall(pattern, content, re.DOTALL)\n",
        "\n",
        "            for match in matches:\n",
        "                start_time, end_time, text = match\n",
        "\n",
        "                # Convert timestamp to seconds\n",
        "                def time_to_seconds(time_str):\n",
        "                    h, m, s = time_str.split(':')\n",
        "                    return float(h) * 3600 + float(m) * 60 + float(s)\n",
        "\n",
        "                segments.append({\n",
        "                    'text': text.strip(),\n",
        "                    'start': time_to_seconds(start_time),\n",
        "                    'end': time_to_seconds(end_time)\n",
        "                })\n",
        "\n",
        "        return segments\n",
        "\n",
        "    def detect_offensive_words_in_segment(self, segment):\n",
        "        \"\"\"\n",
        "        Detect offensive words in a segment of text\n",
        "\n",
        "        Args:\n",
        "            segment: Dictionary containing text and timestamps\n",
        "\n",
        "        Returns:\n",
        "            List of offensive words with their timestamps\n",
        "        \"\"\"\n",
        "        text = segment['text']\n",
        "        start_time = segment['start']\n",
        "        end_time = segment['end']\n",
        "        segment_duration = end_time - start_time\n",
        "\n",
        "        # Tokenize the text into words\n",
        "        words = self.word_tokenize(text.lower())\n",
        "\n",
        "        # Get original words with case preserved\n",
        "        original_words = self.word_tokenize(text)\n",
        "\n",
        "        # Process each word\n",
        "        offensive_words = []\n",
        "\n",
        "        for i, (word_lower, word_original) in enumerate(zip(words, original_words)):\n",
        "            if len(word_lower) <= 2 or not any(c.isalpha() for c in word_lower):\n",
        "                continue\n",
        "\n",
        "            # Get context (up to 3 words before and after)\n",
        "            start_idx = max(0, i - 3)\n",
        "            end_idx = min(len(words), i + 4)\n",
        "            context = \" \".join(words[start_idx:end_idx])\n",
        "\n",
        "            # Use model to predict if the word is offensive\n",
        "            inputs = self.tokenizer(context, return_tensors=\"pt\")\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = self.model(**inputs).logits\n",
        "\n",
        "            # Get prediction score\n",
        "            scores = torch.softmax(logits, dim=1)[0].tolist()\n",
        "            offensive_score = scores[1]  # Probability of being offensive\n",
        "\n",
        "            if offensive_score >= self.threshold:\n",
        "                # Estimate timestamp for this word\n",
        "                # Simple approach: assume words are evenly distributed in segment\n",
        "                word_position = i / len(words)\n",
        "                word_time = start_time + (segment_duration * word_position)\n",
        "\n",
        "                offensive_words.append({\n",
        "                    'word': word_original,\n",
        "                    'confidence': offensive_score,\n",
        "                    'timestamp': word_time,\n",
        "                    'segment_start': start_time,\n",
        "                    'segment_end': end_time,\n",
        "                    'context': context\n",
        "                })\n",
        "\n",
        "        return offensive_words\n",
        "\n",
        "    def process_transcript(self, transcript_file, output_file=None):\n",
        "        \"\"\"\n",
        "        Process a Whisper transcript file to detect offensive words\n",
        "\n",
        "        Args:\n",
        "            transcript_file: Path to the Whisper transcript file\n",
        "            output_file: Path to save the output JSON file (optional)\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with detected offensive words and their timestamps\n",
        "        \"\"\"\n",
        "        # Parse the transcript\n",
        "        segments = self.parse_whisper_transcript(transcript_file)\n",
        "        print(f\"Parsed {len(segments)} segments from transcript\")\n",
        "\n",
        "        # Process each segment\n",
        "        all_offensive_words = []\n",
        "\n",
        "        for i, segment in enumerate(segments):\n",
        "            print(f\"Processing segment {i+1}/{len(segments)}\")\n",
        "            offensive_words = self.detect_offensive_words_in_segment(segment)\n",
        "            all_offensive_words.extend(offensive_words)\n",
        "\n",
        "        # Create result dictionary\n",
        "        result = {\n",
        "            'total_offensive_words': len(all_offensive_words),\n",
        "            'offensive_words': all_offensive_words\n",
        "        }\n",
        "\n",
        "        # Save to file if output_file is provided\n",
        "        if output_file:\n",
        "            with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(result, f, indent=2)\n",
        "            print(f\"Saved results to {output_file}\")\n",
        "\n",
        "        return result\n",
        "\n",
        "# Part 2: Install required packages (Run this cell first)\n",
        "# Only run these if they're not already installed\n",
        "!pip install transformers torch nltk\n",
        "\n",
        "# Part 3: Colab-specific implementation for processing Whisper transcripts\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "def run_transcript_processing():\n",
        "    \"\"\"\n",
        "    Main function to run the Whisper transcript processing in Colab\n",
        "    \"\"\"\n",
        "    # Specify the paths\n",
        "    transcript_file = \"/content/transcription.json\"  # Path to the Whisper transcript file\n",
        "    model_path = \"/content/offensive_word_nlp_model_final\"  # Path to the offensive word detection model\n",
        "    output_file = \"/content/offensive_words.json\"  # Path to save the output JSON file\n",
        "\n",
        "    # Initialize the processor and process the transcript\n",
        "    try:\n",
        "        processor = WhisperTranscriptProcessor(model_path, threshold=0.7)\n",
        "\n",
        "        print(f\"\\nProcessing transcript: {transcript_file}\")\n",
        "        result = processor.process_transcript(transcript_file, output_file)\n",
        "\n",
        "        # Print summary\n",
        "        print(f\"\\nFound {result['total_offensive_words']} offensive words in the transcript\")\n",
        "\n",
        "        # Print first few examples\n",
        "        if result['offensive_words']:\n",
        "            print(\"\\nExamples of detected offensive words:\")\n",
        "            for i, word_info in enumerate(result['offensive_words'][:5]):  # Show first 5 examples\n",
        "                print(f\"{i+1}. '{word_info['word']}' at {word_info['timestamp']:.2f}s (confidence: {word_info['confidence']:.2f})\")\n",
        "                print(f\"   Context: \\\"{word_info['context']}\\\"\")\n",
        "\n",
        "        # Download the output file\n",
        "        print(\"\\nStep 3: Download the results\")\n",
        "        files.download(output_file)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing transcript: {str(e)}\")\n",
        "\n",
        "# Part 4: Run the processing function (Run this cell after the above cells)\n",
        "# Execute this function to start the process\n",
        "if __name__ == \"__main__\":\n",
        "    run_transcript_processing()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "j-0mto-2wNqD",
        "outputId": "98cf932c-f793-4411-cfaf-adc424f66b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punkt data location: /root/nltk_data/tokenizers/punkt\n",
            "punkt_tab file found!\n",
            "Punkt tokenizer loaded successfully!\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Model loaded successfully and moved to cuda\n",
            "\n",
            "Processing transcript: /content/transcription.json\n",
            "Parsed 9 segments from transcript\n",
            "Processing segment 1/9\n",
            "Processing segment 2/9\n",
            "Processing segment 3/9\n",
            "Processing segment 4/9\n",
            "Processing segment 5/9\n",
            "Processing segment 6/9\n",
            "Processing segment 7/9\n",
            "Processing segment 8/9\n",
            "Processing segment 9/9\n",
            "Saved results to /content/offensive_words.json\n",
            "\n",
            "Found 70 offensive words in the transcript\n",
            "\n",
            "Examples of detected offensive words:\n",
            "1. 'all' at 0.46s (confidence: 0.99)\n",
            "   Context: \"you 're all you raggedy ass\"\n",
            "2. 'you' at 0.69s (confidence: 1.00)\n",
            "   Context: \"you 're all you raggedy ass niggas\"\n",
            "3. 'raggedy' at 0.92s (confidence: 1.00)\n",
            "   Context: \"'re all you raggedy ass niggas and\"\n",
            "4. 'ass' at 1.15s (confidence: 1.00)\n",
            "   Context: \"all you raggedy ass niggas and man-hows\"\n",
            "5. 'niggas' at 1.38s (confidence: 1.00)\n",
            "   Context: \"you raggedy ass niggas and man-hows worth\"\n",
            "\n",
            "Step 3: Download the results\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0d79a454-e6a4-49b8-8e5b-871c8a01eb64\", \"offensive_words.json\", 15600)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}